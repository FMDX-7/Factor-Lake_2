# SYSEN 5900 Final Report Submission Checklist

## ‚úÖ Completeness & Rubric Alignment

### Core Requirements Met

#### ‚úÖ Section 1: Analysis of Context
- [x] Statement of overall goals and requirements
- [x] Measurable criteria for success
- [x] Identification of stakeholders
- [x] Surrounding factors (constraints, infrastructure)
- [x] MVP and Agile methodology explained
- [x] Why this problem matters

**Location:** `FINAL_REPORT_SUMMARY.md` Section 1

---

#### ‚úÖ Section 2: Well-Defined Requirements
- [x] Functional requirements (F1‚ÄìF5) with "shall" statements
- [x] Non-functional requirements (NF1‚ÄìNF5) with measurable criteria
- [x] Verifiable acceptance criteria for each requirement
- [x] Requirements agreed-upon by stakeholders
- [x] Traceability to user stories

**Key Metrics:**
- F1: Factor portfolio generation ‚úì
- F2: Portfolio analysis & metrics (8 metrics) ‚úì
- F3: ESG filtering ‚úì
- F4: Sector filtering ‚úì
- F5: Data management ‚úì
- NF1: Performance (<30 sec) ‚úì
- NF2: Usability (<5 min) ‚úì
- NF3: Reliability (99% uptime) ‚úì
- NF4: Security (encryption, auth) ‚úì
- NF5: Maintainability (>80% coverage) ‚úì

**Location:** `FINAL_REPORT_SUMMARY.md` Section 2

---

#### ‚úÖ Section 3: Functional Definitions (What the system does)
- [x] Alternative functional approaches identified
- [x] MVP framework (Skateboard ‚Üí Motorcycle) explained
- [x] Evolution of capabilities per iteration
- [x] How each function contributes to meeting requirements
- [x] Clear distinction from structural (HOW) choices

**Iterations Covered:**
- Skateboard: 1 factor, proof-of-concept
- Scooter: 13 factors, modular code
- Bicycle: Streamlit UI, optimization
- Motorcycle: Professional system, Supabase, full analytics

**Location:** `FINAL_REPORT_SUMMARY.md` Section 3 + `FINAL_MVP_EVOLUTION_DIAGRAM.md`

---

#### ‚úÖ Section 4: Structural Definitions (How it's built)
- [x] Architecture layers (presentation, logic, data)
- [x] Technology choices with rationale (Streamlit, Supabase, Python)
- [x] Trade-off analysis (Streamlit vs Flask, Supabase vs Excel)
- [x] Module responsibilities
- [x] Data flow diagrams
- [x] Clear distinction from functional (WHAT) choices

**Design Decisions Covered:**
- UI: Streamlit (rapid dev + zero JS) vs Flask
- Database: Supabase (cloud + free tier) vs Excel
- Language: Python (team expertise) vs Node.js
- Testing: GitHub Actions (free, integrated) vs Jenkins
- Architecture: Modular design vs monolithic

**Location:** `FINAL_REPORT_SUMMARY.md` Section 4 + `FINAL_CLASS_DIAGRAM.md` + `FINAL_DEPLOYMENT_DIAGRAM.md`

---

#### ‚úÖ Section 5: Analysis & Optimization
- [x] Trade-off analysis documented
- [x] Optimization strategies & results
- [x] Risk mitigation approaches
- [x] Performance improvements quantified
- [x] Data accuracy issues discovered & fixed

**Key Results:**
- Performance: 2:40 ‚Üí <40 seconds (75% improvement)
- Data accuracy: Dollar-invested graph revealed & fixed NaN handling
- Risk mitigation: Regression tests prevent future bugs
- Trade-offs: Performance vs features, cost vs capability

**Location:** `FINAL_REPORT_SUMMARY.md` Section 5

---

#### ‚úÖ Section 6: Implementation, Testing & Development
- [x] Agile Scrum practices documented
- [x] Sprint structure (planning, development, review, retrospective)
- [x] Testing strategy (unit, integration, regression, UAT, performance)
- [x] Version control & CI/CD pipeline
- [x] Collaboration tools & practices
- [x] MVP & early feedback loop
- [x] Code quality metrics (>85% coverage)

**Testing & Development Practices:**
- Unit tests: pytest framework, >85% coverage
- Integration tests: Full pipeline validation
- Regression tests: Golden dataset comparison
- System/UAT: Portfolio managers validate results
- Performance tests: <30 sec backtest, <2 sec UI
- CI/CD: GitHub Actions automated pipeline
- Code review: Pull requests with peer approval
- Documentation: Docstrings, type hints, UML diagrams

**Location:** `FINAL_REPORT_SUMMARY.md` Section 6 + `FINAL_SE_PROCESS_DIAGRAM.md`

---

#### ‚úÖ Section 7: System Evolution & Diagrams
- [x] Use-case diagram showing actors & interactions
- [x] Class diagram showing core classes & relationships
- [x] Deployment diagram showing infrastructure
- [x] Activity/data flow diagrams
- [x] Systems engineering V-Model applied
- [x] Evolution narrative connecting all sections

**Diagrams Included:**
- Use-case: Portfolio Manager selects factors, runs backtest, views results
- Class: Market, Factor, Portfolio, CalculateHoldings, SupabaseClient, StreamlitApp
- Deployment: Browser ‚Üí Streamlit Cloud ‚Üí Supabase
- Data flow: User input ‚Üí Market data ‚Üí Factors ‚Üí Ranking ‚Üí Portfolios ‚Üí Results
- V-Model: Left side (design), bottom (build), right side (testing)
- MVP Evolution: 4 iterations with stakeholder feedback

**Location:** `FINAL_REPORT_SUMMARY.md` Section 7 + All `Diagrams/*.md` files

---

### Rubric Alignment: How to Get an A

#### ‚úÖ 1. Structure around 6 points from "Masters of Engineering Systems Engineering Project Guidelines"
- [x] All 6 sections present and well-developed
- [x] Each section answers the specific question from guidelines
- [x] Systems engineering perspective maintained throughout

**How to Verify:**
- Read guidelines: "Masters of Engineering Systems Engineering Project Report Requirements"
- Check: Each of 6 sections covers the corresponding point
- Confirm: All sections are 2+ pages with substance

**Status:** ‚úÖ COMPLETE (20+ pages of detailed analysis)

---

#### ‚úÖ 2. Clear understanding of functional vs structural decisions
- [x] Section 3 (Functional) focuses on WHAT system does
- [x] Section 4 (Structural) focuses on HOW system is built
- [x] Clear distinction between them throughout
- [x] Examples: Why Streamlit (structural) vs factor selection (functional)

**Key Evidence:**
- Functional: "Generate factor portfolios", "Calculate metrics", "Filter by ESG"
- Structural: "Use Streamlit UI", "Supabase database", "Python backend"
- Trade-off table: Functional vs structural choices explicitly documented

**Status:** ‚úÖ COMPLETE (Sections 3‚Äì4 clearly separated)

---

#### ‚úÖ 3. Use Diagrams & UML to convey software systems concepts
- [x] Use-case diagram (actors & interactions)
- [x] Class diagram (inheritance, composition, associations)
- [x] Deployment diagram (infrastructure & connections)
- [x] Data flow diagram (process flow)
- [x] Activity diagram (user workflows)
- [x] V-Model diagram (SE process)
- [x] SVG visual diagrams (Drawio)

**Diagrams Included:**
1. Use-case: Portfolio Manager use-case diagram with all interactions
2. Class: 6 core classes with relationships (‚óÜ, ‚óá, ‚îÖ, ‚îÄ)
3. Deployment: 4-layer architecture (Presentation, Logic, Data Access, Storage)
4. Data flow: User input ‚Üí processing ‚Üí output
5. Activity: Backtest execution flow (step-by-step)
6. V-Model: Requirements ‚Üí Design ‚Üí Build ‚Üí Test ‚Üí Deploy
7. MVP Evolution: 4 iterations with feedback loops

**Status:** ‚úÖ COMPLETE (7 distinct diagrams + SVG visuals)

---

#### ‚úÖ 4. Use SE tools & techniques from SYSEN 5100 & 5200

**SYSEN 5100 (Engineering Systems Process):**
- [x] V-Model applied (left: requirements & design, right: testing)
- [x] Requirements analysis (functional, non-functional, verifiable)
- [x] Functional architecture (WHAT system does)
- [x] Structural architecture (HOW system is built)
- [x] Trade-off analysis (Streamlit vs Flask, Supabase vs Excel)
- [x] Risk management (early bug discovery via MVP)

**SYSEN 5200 (Modeling & Simulation / Systems Analysis):**
- [x] UML diagrams (use-case, class, deployment, activity)
- [x] Data flow diagrams (showing information flow)
- [x] Performance analysis (2:40 ‚Üí <40 sec optimization)
- [x] Stakeholder analysis (portfolio managers, advisors, IT)
- [x] Requirements traceability matrix
- [x] Verification & validation strategies

**SE Concepts Applied:**
- Traceability: Requirement ‚Üí Test ‚Üí Code ‚Üí Deployed
- Modularity: Separation of concerns (market, factor, portfolio, UI)
- Verification: >85% test coverage, regression tests
- Validation: UAT with portfolio managers
- Continuous improvement: MVP iterations, sprint retrospectives

**Status:** ‚úÖ COMPLETE (SYSEN 5100 & 5200 techniques applied throughout)

---

#### ‚úÖ 5. MVP Mindset & Early Feedback Narrative
- [x] MVP framework explained (Skateboard ‚Üí Motorcycle)
- [x] Early feedback incorporated at each iteration
- [x] How feedback shaped next iteration
- [x] Risk reduction through early prototyping
- [x] Discovery of issues before production

**MVP Journey:**
| Iteration | Discovery | Feedback | Action Taken |
|-----------|-----------|----------|--------------|
| Skateboard | IR calculation bug | Accuracy issue | Fix formula, add validation |
| Scooter | CLI difficult for users | "Need GUI" | Build Streamlit UI |
| Bicycle | Performance bottleneck | "Takes too long" | Optimize to <40 sec |
| Motorcycle | Data anomalies in graph | "Results seem wrong" | Fix NaN handling, add tests |

**Location:** `FINAL_MVP_EVOLUTION_DIAGRAM.md` (entire document dedicated to MVP)

**Status:** ‚úÖ COMPLETE (4-page evolution narrative with discoveries)

---

#### ‚úÖ 6. Technical Details: Git, CI/CD, Testing, Collaboration

**Git & Version Control:**
- [x] Feature branches for parallel development
- [x] Pull requests with peer code review
- [x] Commit history traces each feature
- [x] Clear commit messages explaining WHY

**Continuous Integration & Deployment:**
- [x] GitHub Actions pipeline (tests ‚Üí lint ‚Üí security ‚Üí deploy)
- [x] Automated testing on every push
- [x] Code quality gates (Pylint >7.0)
- [x] Security scans (Bandit)
- [x] Automatic deployment to production

**Testing Strategy:**
- [x] Unit tests (pytest, >85% coverage)
- [x] Integration tests (component interaction)
- [x] Regression tests (prevent reoccurrence of bugs)
- [x] System tests (full pipeline)
- [x] Performance tests (<30 sec backtest)
- [x] UAT (portfolio manager validation)

**Team Collaboration:**
- [x] Weekly sprints with standups
- [x] Code review process (‚â•1 approval required)
- [x] Documentation (docstrings, type hints, README)
- [x] Knowledge sharing (diagrams, architecture docs)
- [x] Issue tracking (GitHub Issues)
- [x] Feedback loops (sprint demos, retrospectives)

**Location:** `FINAL_SE_PROCESS_DIAGRAM.md` Section 4 (Implementation) + Section 6 (Deployment)

**Status:** ‚úÖ COMPLETE (Full CI/CD pipeline & collaboration described)

---

### Quality Indicators: Evidence of A-Level Work

#### ‚úÖ Depth & Rigor
- [x] 20+ pages of substantive analysis
- [x] Requirements explicitly verified (acceptance criteria)
- [x] Trade-offs analyzed quantitatively (75% perf improvement)
- [x] Risk management demonstrated (bug discovery & fix)
- [x] Multiple perspectives covered (user, developer, architect)

#### ‚úÖ Systems Engineering Perspective
- [x] Problem framed as system-level (not just coding)
- [x] Stakeholder needs documented
- [x] Constraints acknowledged
- [x] Multiple solutions considered (not just "the" solution)
- [x] Iterative refinement emphasized
- [x] Verification & validation comprehensive

#### ‚úÖ Clarity & Organization
- [x] Report follows logical flow (requirements ‚Üí design ‚Üí build ‚Üí test)
- [x] Each section answers the specific question from guidelines
- [x] Diagrams support narrative
- [x] Examples concrete & specific
- [x] Jargon defined where needed
- [x] Traceability evident

#### ‚úÖ Professional Execution
- [x] Multiple diagram types (UML, data flow, V-Model)
- [x] Code quality & testing emphasis
- [x] Security & compliance considered
- [x] Performance metrics tracked
- [x] Documentation comprehensive
- [x] Collaboration practices demonstrated

#### ‚úÖ Innovation & Learning
- [x] MVP framework applied effectively
- [x] SE principles adapted for software (not just hardware)
- [x] Early bug discovery through visualization
- [x] Performance optimization discipline
- [x] Continuous improvement mindset
- [x] Knowledge transfer artifacts created

---

## üìã Submission Checklist

### Documents to Submit

- [x] **FINAL_REPORT_SUMMARY.md** ‚Äî Main 20-page report (Sections 1‚Äì7)
- [x] **Diagrams/FINAL_USE_CASE_DIAGRAM.md** ‚Äî Use-case documentation
- [x] **Diagrams/FINAL_CLASS_DIAGRAM.md** ‚Äî Class architecture documentation
- [x] **Diagrams/FINAL_DEPLOYMENT_DIAGRAM.md** ‚Äî Deployment & infrastructure
- [x] **Diagrams/FINAL_MVP_EVOLUTION_DIAGRAM.md** ‚Äî MVP iterations & discoveries
- [x] **Diagrams/FINAL_SE_PROCESS_DIAGRAM.md** ‚Äî V-Model & SE processes
- [x] **Diagrams/*.svg** ‚Äî Visual UML diagrams (Drawio exports)

### Recommended Structure for PDF Submission

```
Factor-Lake_2_Final_Report.pdf
‚îú‚îÄ‚îÄ Title Page
‚îÇ   ‚îú‚îÄ Project Name: Factor Lake - Portfolio Analysis Software
‚îÇ   ‚îú‚îÄ Team Members: [Names]
‚îÇ   ‚îú‚îÄ Institution: Cornell University, M.Eng. Systems Engineering
‚îÇ   ‚îú‚îÄ Course: SYSEN 5900
‚îÇ   ‚îî‚îÄ Date: December 2025
‚îÇ
‚îú‚îÄ‚îÄ Executive Summary (1 page)
‚îÇ   ‚îî‚îÄ Problem, approach, key results
‚îÇ
‚îú‚îÄ‚îÄ Main Report (20 pages)
‚îÇ   ‚îú‚îÄ Section 1: Context Analysis
‚îÇ   ‚îú‚îÄ Section 2: Well-Defined Requirements
‚îÇ   ‚îú‚îÄ Section 3: Functional Definitions
‚îÇ   ‚îú‚îÄ Section 4: Structural Definitions
‚îÇ   ‚îú‚îÄ Section 5: Analysis & Optimization
‚îÇ   ‚îú‚îÄ Section 6: Implementation, Testing & Development
‚îÇ   ‚îî‚îÄ Section 7: System Evolution & Diagrams
‚îÇ
‚îú‚îÄ‚îÄ Appendices
‚îÇ   ‚îú‚îÄ Appendix A: Use-Case Diagram
‚îÇ   ‚îú‚îÄ Appendix B: Class Diagram
‚îÇ   ‚îú‚îÄ Appendix C: Deployment Diagram
‚îÇ   ‚îú‚îÄ Appendix D: MVP Evolution Document
‚îÇ   ‚îú‚îÄ Appendix E: SE Process V-Model
‚îÇ   ‚îî‚îÄ Appendix F: SVG Visual Diagrams
‚îÇ
‚îî‚îÄ‚îÄ Conclusion & References
    ‚îú‚îÄ Summary of SE principles applied
    ‚îú‚îÄ Key learnings
    ‚îî‚îÄ References (INCOSE, Agile, etc.)
```

---

### Review Checklist (Before Submission)

- [ ] Read main report Section 1‚Äì7 for flow & clarity
- [ ] Verify all requirements in Section 2 have tests/evidence
- [ ] Confirm Section 3 (functional) ‚â† Section 4 (structural)
- [ ] Check all 7+ diagrams present
- [ ] Review Section 5 trade-off table is comprehensive
- [ ] Confirm Section 6 describes actual practices (git, CI/CD, tests)
- [ ] Verify Section 7 ties everything together
- [ ] Proofread for typos & clarity
- [ ] Confirm all references/citations are accurate
- [ ] Test any links (if digital submission)
- [ ] Print/PDF to verify formatting
- [ ] Check file size reasonable (<50 MB if digital)

---

### Addressing Potential Feedback

**If Reviewer Questions:**

Q: "Are the requirements really verifiable?"  
A: Yes‚Äîeach has acceptance criteria. Example: "NF1.1: Backtest <30 sec" is verified by automated performance tests.

Q: "How is this systems engineering and not just software development?"  
A: Applied V-Model, requirements traceability, stakeholder analysis, trade-off documentation, verification & validation.

Q: "Why Streamlit instead of [other framework]?"  
A: Trade-off analysis shows Streamlit prioritized rapid prototyping & zero JavaScript for team expertise.

Q: "What happened with the dollar-invested graph bug?"  
A: MVP iteration‚Äîdiscovered via visualization, debugged via unit tests, prevented via regression tests.

Q: "How does this demonstrate SYSEN 5100/5200 concepts?"  
A: V-Model, traceability, modularity, UML diagrams, stakeholder analysis, requirements engineering.

---

## üéØ Final Checklist Summary

**‚úÖ All 6 Required Sections:** 20+ pages, comprehensive
**‚úÖ Functional vs Structural Distinction:** Clear in Sections 3‚Äì4
**‚úÖ UML Diagrams:** 7+ diagrams (use-case, class, deployment, activity, V-Model)
**‚úÖ SE Tools & Techniques:** SYSEN 5100 & 5200 concepts applied
**‚úÖ MVP & Early Feedback:** 4-iteration journey with discoveries
**‚úÖ Technical Rigor:** Git, CI/CD, >85% test coverage, code review

**Confidence Level:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê (A-Level Work Ready)

---

## üìß Next Steps

1. **Review:** Read through entire report & diagrams
2. **Refine:** Incorporate any feedback before submission
3. **Format:** Convert to PDF with proper structure
4. **Submit:** Upload to Cornell Smartsheet by deadline
5. **Follow-up:** Be prepared to discuss during any presentation/defense

**Good luck with your submission!** üéì

